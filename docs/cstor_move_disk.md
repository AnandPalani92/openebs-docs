# Move Disk to New Node
1. Detach the disk from the old node (if necessary) and attach to the new node.  This will be different based on where your Kubernetes cluster is running.  For on premises installations, follow your normal procedures for moving block storage.  For the cloud, follow the documentation provided by your cloud provider.

2. Edit the YAML file for your StoragePoolClaim to increment the maxPools count by 1.

	```
	$ kubectl apply -f <SPC_YAML_FILE>
	storagepoolclaim.openebs.io/cstor-test configured
	```

3. Wait for the new pool to be created. Find the new storage pool.

	```
	$ kubectl get csp
	NAME              ALLOCATED   FREE    CAPACITY   STATUS    TYPE      AGE
	cstor-test-jc1m   16.3M       49.7G   49.8G      Healthy   striped   11m
	cstor-test-l8jb   16.3M       49.7G   49.8G      Healthy   striped   11m
	cstor-test-p7mq   16.3M       49.7G   49.8G      Healthy   striped   11m
	cstor-test-uko9   77K         49.7G   49.8G      Healthy   striped   50s
	```

4. Find the hostname and uid for the new pool

	```
	kubectl get csp <POOL_NAME> -o yaml | grep "\(hostname\|uid\)"
	    kubernetes.io/hostname: k8s-openebs-demo-node-3
	    uid: 645dfc1c-883d-11e9-afbe-42010a800fc7
	  uid: 4f477f5f-8846-11e9-afbe-42010a800fc7
	```

5. Find the cvr that needs to be moved.  It will have the pool name as the second half of it's name.

	```
	$ kubectl -n openebs get cvr
	NAME                                                       USED    ALLOCATED   STATUS    AGE
	pvc-739efa06-884c-11e9-a4c1-42010a800fc3-cstor-test-jc1m   56.0M   15.8M       Healthy   11m
	pvc-739efa06-884c-11e9-a4c1-42010a800fc3-cstor-test-l8jb   56.0M   15.8M       Healthy   11m
	pvc-739efa06-884c-11e9-a4c1-42010a800fc3-cstor-test-p7mq   56.0M   15.8M       Healthy   11m
	```

6. Get the yaml definition of the CVR and save it somewhere.

	```
	$ kubectl -n openebs get cvr <CVR_NAME> -o yaml > /tmp/<CVR_NAME>.yaml
	```

7. Now edit the YAML file we just created to remove any autogenerated fields (creationTimestamp, selfLink, uid, state, etc).  Then update the hostname annotation, the label `cstorpool.openebs.io/uid` and the CVR's name.  It should look something like this:

	```
	apiVersion: openebs.io/v1alpha1
	kind: CStorVolumeReplica
	metadata:
	  annotations:
	    cstorpool.openebs.io/hostname: k8s-openebs-demo-node-3
	    isRestoreVol: "false"
	    openebs.io/storage-class-ref: |
	      name: openebs-cstor-test
	      resourceVersion: 3003
	  labels:
	    cstorpool.openebs.io/name: cstor-test-uko9
	    cstorpool.openebs.io/uid: 4f477f5f-8846-11e9-afbe-42010a800fc7
	    cstorvolume.openebs.io/name: pvc-739efa06-884c-11e9-a4c1-42010a800fc3
	    openebs.io/cas-template-name: cstor-volume-create-default-0.9.0
	    openebs.io/persistent-volume: pvc-739efa06-884c-11e9-a4c1-42010a800fc3
	    openebs.io/version: 0.9.0
	  name: pvc-739efa06-884c-11e9-a4c1-42010a800fc3-cstor-test-uko9
	  namespace: openebs
	spec:
	  capacity: 8G
	  targetIP: 10.101.169.188
	  zvolWorkers: ""
	```

8. Delete the old CVR.

	```
	$ kubectl -n openebs delete cvr <OLD_CVR_NAME>
	cstorvolumereplica.openebs.io "pvc-739efa06-884c-11e9-a4c1-42010a800fc3-cstor-test-jc1m" deleted
	```

9. Create the new CVR.

	```
	$ kubectl apply -f /tmp/pvc-739efa06-884c-11e9-a4c1-42010a800fc3-cstor-test-jc1m.yaml 
	cstorvolumereplica.openebs.io/pvc-739efa06-884c-11e9-a4c1-42010a800fc3-cstor-test-uko9 created
	```

10. Describe the new CVR to check it's status.

	```
	$ kubectl -n openebs describe cvr <NEW_CVR_NAME>
	```